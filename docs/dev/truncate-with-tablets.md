# Background

The CQL TRUNCATE query is obviously data-modifying since it deletes user data, however,
unlike DELETE queries, it is not based on a tombstone that carries a write-timestamp,
with which we can determine for each cell, if it was logically written before
or after the tombstone timestamp.

Instead, truncate propagates in the cluster using an out-of-band RPC and is currently
applied locally in each node using the local wall clock of each node (not synchronized
with each other), typically after flushing the tables and taking a snapshot of them,
if auto-snapshot is enabled.

This presents a problem when tablets are enabled since there could be on-going tablet
transitions in the background, that the user has no control over, like tablet migration,
with which a tablet might escape truncate by migrating between nodes just at the wrong
time so it won’t be considered for truncate by any node.

This document lists the currently proposed solutions and lists the selected proposal.

# Proposed solutions

## 1. Short term mitigation (disable/enable load balancing)

We can disable any background load balancing operations (if any are running) and execute the
truncate as it is currently implemented. We would issue a RPC which performs a memtable flush,
then take a snapshot of the SSTables if auto-snapshot is enabled, and delete the SSTables
belonging to the given table and associated views. After truncate is complete, we should
re-enable load balancing.

A problem with this solution is that a truncate operation which is issued while another
truncate is already running could potentially run into the same problem with resurecting
data which is in migration. Consider the following scenario:

- a truncate table command is received (TT1)
- load balancing is disabled as a result of TT1
- truncating the table and views for TT1 is started
- another truncate table command is received (TT2)
- TT2 does not disable load balancing, as it is already disabled
- truncating the table for TT1 completes 
- TT1 enables load balancing
- TT2 starts executing the truncate operation while load balancing is enabled potentially
causing the problem with tablet migration resurecting truncated data

Another potential problem is that load balancing can be manually enabled by the admin while
truncate is still running, causing the same problem with resurrected data due to migration.

If the node which controls the truncate operation crashes before truncate is complete,
it will leave load balancing disabled.

## 2. Short term mitigation (holding ERM pointer)

Holding an effective replication map pointer on any node will pause load balancing until
the pointer is released. Therefor, obtaining an ERM pointer before truncate starts will
pause load balancing operations until truncate is complete, when the ERM pointer can be
released, which will resume load balancing.

This solution does not suffer from the problems with concurrent truncate operations, or
with manual load balancing disable/enable by an admin.

## 3. Truncate as a topology operation

We need a barrier that will wait on any on-going tablet transitions on the truncated table
AND on its subordinate views (materialized views and secondary indexes), and prevent any new
ones from taking place, similar to safe repair.

At this point we can build a transition plan, using a new, “truncate” tablet transition type,
for all tablet replicas for the truncate table and its view.

Open questions: should flush and snapshot happen as they do today, before preparing and executing
the truncate transition plan, or should they be performed inline, by each tablet, as it is truncated.
For the latter, we’ll need to pass the snapshot tag (a.k.a name) so the distributed snapshots will
use the same tag. This is harder, but we want to support a distributed snapshot anyhow in the future
for integrating backup with tablets (and the object storage stack).

## 4. Truncate with tombstones stored in SSTables

We need to implement a new type of token-range tombstone which will be stored in SSTables.
The advantage of this solution is that it fits well into the current tombstone logic since we already
have several kinds of tombstones: cell, record, range and partition. However, this also requires
changes in many places in the current code:

- a new mutation fragment type
- the commit log
- row cache
- memtable
- SSTable writers and readers

Also, matching and filtering of SSTables will need to change.

While this solution seems to require the most amount of work, it has an advantage that token
range tombstones have a wider applicability. However, further discussion is needed to clarify
the details about the use of these tombstones, how they should look like and how they should
propagate.

## 5. Truncate with tombstones stored in table schema

With this solution we would store the tombstone in the table schema. The advantages are that
it would  be easier to integrate with the readers since they already have access to the schema.
And it could be propagated with the current schema change mechanisms. This solution allows
incorporating into the backup and restore procedure so that a table can be safely backed up
during truncate. This solution does not require all nodes to be up.

# Accepted solution

The __Short term mitigation (holding ERM pointer)__ solution was chosen because of simplicity
of implementation.

All other proposed solutions need further discussion to clarify the details of implementation
and find any remaining potential pitfalls.
